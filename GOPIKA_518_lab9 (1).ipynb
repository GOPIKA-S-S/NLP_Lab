{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxpS5AVqG5VW",
        "outputId": "ba16106d-e353-46a4-b41a-2cc3e033feb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6666666666666667\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Load the spaCy English tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def edit_distance_nlp(s1, s2):\n",
        "    doc1 = nlp(s1)\n",
        "    doc2 = nlp(s2)\n",
        "\n",
        "    tokens1 = [token.text for token in doc1]\n",
        "    tokens2 = [token.text for token in doc2]\n",
        "\n",
        "    # Calculate the similarity score based on the length of the matching subsequence\n",
        "    sequence_matcher = SequenceMatcher(None, tokens1, tokens2)\n",
        "    similarity_score = sequence_matcher.ratio()\n",
        "\n",
        "    # Normalize the similarity score to obtain the edit distance\n",
        "    edit_distance = 1.0 - similarity_score\n",
        "    return edit_distance\n",
        "\n",
        "# Test the edit_distance_nlp function\n",
        "text1 = \"natural language processing is fascinating\"\n",
        "text2 = \"I am intrigued by natural language understanding\"\n",
        "print(edit_distance_nlp(text1, text2))  # Output: 0.6190476190476191\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqHWrNM_HcgO",
        "outputId": "3d9277f9-5ed3-447a-cc38-6c637a9183e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ap3wgwG8UD",
        "outputId": "75745de4-4132-4c50-af55-b4efa9f6096d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I walked through the forest and felt the forest in the air. I walked through the trees and felt the trees in the air. I walked through the birds and felt the birds in the air.\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to generate a story or poem based on keywords\n",
        "def generate_story_or_poem(keywords):\n",
        "    doc = nlp(keywords)\n",
        "\n",
        "    # Extract nouns from keywords\n",
        "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "\n",
        "    # Generate sentences based on the extracted nouns\n",
        "    sentences = []\n",
        "    for noun in nouns:\n",
        "        sentence = f\"I walked through the {noun} and felt the {noun} in the air.\"\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "# Keywords for the story or poem\n",
        "keywords = \"forest trees birds\"\n",
        "\n",
        "# Generate the story or poem based on the keywords\n",
        "generated_text = generate_story_or_poem(keywords)\n",
        "print(generated_text)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}